import json
import re
import nltk
import random
import time
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords,wordnet
from nltk.stem import WordNetLemmatizer
from gensim.models import LdaModel,TfidfModel,LsiModel
from gensim.models.phrases import Phrases,Phraser
from gensim.models.coherencemodel import CoherenceModel
from gensim.corpora import Dictionary
from multiprocessing import Pool, cpu_count

def preprocess_parallel(dataset, stop_words):
    #dataset - list of complaints
    #stop_words - tokens that won't be used for analysis
    #
    # Create pool for all functions with parallel processing using all cores except 1. Call the preprocess function.
    pool = Pool(processes=cpu_count()-1)
    results = pool.starmap(preprocess, [(doc, stop_words) for doc in dataset])
    pool.close()
    pool.join()
    return results

def compute_optimal_topics_parallel(tokens, dictionary, corpus, start, limit, step, method):
    #tokens - list of tokens generated by the preprocessing function
    #dictionary - dictionary created during vectorization
    #corpus - corpus created during vectorization
    #start - Int topic count to start coherence calculation
    #limit - Int topic count to stop coherence calculation
    #step - Int stepsize for coherence calculation
    #method - topic extraction method to use (1=lsa,2=lda)
    #
    # Create pool for all functions with parallel processing using all cores except 1. Call the compute_optimal_topics function
    pool = Pool(processes=cpu_count()-1)
    coherence_values = pool.starmap(compute_optimal_topics,
        [(tokens, dictionary, corpus, num_topics, method) for num_topics in range(start, limit + 1, step)]
        )
    x = list(range(start, limit + 1, step))
    #Return the number of topics with the highest coherence value
    print("Choosing " + str(x[coherence_values.index(max(coherence_values))]) + " topics since it has the highest coherence score")
    return x[coherence_values.index(max(coherence_values))],coherence_values
    
def remove_urls(dataset: str):
    #dataset - list of complaints
    
    # Regex pattern to match URLs (http, https, www)
    url_pattern = r'http\S+|www\.\S+'
    return re.sub(url_pattern, '', dataset)

def preprocess (dataset,stop_words):
    #dataset - list of complaints
    #stop_words - tokens that won't be used for analysis
    
    #Initialize Lemmatizer
    lemmatizer = WordNetLemmatizer()
    def lemmatize_tokens(tokens):
    # convert POS tag to WordNet format
        def get_wordnet_pos(word):
            tag = nltk.pos_tag([word])[0][1][0].upper()
            tag_dict = {"J": wordnet.ADJ,
                        "N": wordnet.NOUN,
                        "V": wordnet.VERB,
                        "R": wordnet.ADV}
            return tag_dict.get(tag, wordnet.NOUN)
        
        # lemmatize tokens
        lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]  
        return lemmas

    #Convert to lowercase
    preprocdataset = dataset.lower()
    #Remove URLs from dataset
    preprocdataset = remove_urls(preprocdataset)
    #Remove special chars and digits
    preprocdataset = re.sub(r'[^A-Za-z\s]', '', preprocdataset)
    #Tokenize string
    preprocdataset = word_tokenize(preprocdataset)
    #Remove Stopwords
    preprocdataset = [word for word in preprocdataset if word not in stop_words]
    #Lemmatization
    preprocdataset = lemmatize_tokens(preprocdataset)
    return preprocdataset

def newgensim_matrix(docs: list, method, min_df: int = 2):
    #Create gensim dictionary + corpus with BoW or TF-IDF
    #docs - list of tokens
    #method - vectorization method (1=BoW,2=TF-IDF)
    #min_df - Int count of documents a bigram has to appear to be classified as a bigram
    
    # Build bigrams
    bigram = Phrases(docs, min_count=min_df, delimiter='_')
    bigram_mod = Phraser(bigram)
    tokens_with_bigrams = [bigram_mod[doc] for doc in docs]

    # Build dictionary
    dictionary = Dictionary(tokens_with_bigrams)
    dictionary.filter_extremes(no_below=min_df, no_above=1.0)

    # Build BoW corpus
    bow_corpus = [dictionary.doc2bow(doc) for doc in tokens_with_bigrams]

    #if method TF-IDF convert BoW to TF-IDF before returning
    if method == 1:
        return dictionary, bow_corpus, tokens_with_bigrams
    elif method == 2:
        tfidf = TfidfModel(bow_corpus)
        tfidf_corpus = tfidf[bow_corpus]
        return dictionary, tfidf_corpus, tokens_with_bigrams
    else:
        raise ValueError("method must be 'bow' or 'tfidf'")

def compute_optimal_topics(tokens, dictionary, corpus, num_topics, method):
    #tokens - list of tokens generated by the preprocessing function
    #dictionary - dictionary created during vectorization
    #corpus - corpus created during vectorization
    #num_topics - Int number of topics to calculate the coherence score
    #method - topic extraction method to use (1=lsa,2=lda)
    
    #Generate LSA or LDA model
    if method == 1:
        model = LsiModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics
        )
    elif method == 2:
        model = LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=num_topics,
            random_state=42,
            passes=10
        )  
    else:
        raise ValueError("method must be 'lsa' or 'lda'")

    # Genterate Coherencce Model 
    # Always use processes=1 so no "daemonic processes are not allowed to have children" exception occurs, because this function is called by a multiprocessing function
    coherencemodel = CoherenceModel(
        model=model,
        texts=tokens,
        dictionary=dictionary,
        coherence="c_v",
        processes=1
    )
    coherence = coherencemodel.get_coherence()
    print(f"Topics = {num_topics}, Coherence = {coherence:.4f}")
    return coherence

def topic_extraction(dictionary, corpus, topiccount, relevanttopic, method):
    #dictionary - dictionary created during vectorization
    #corpus - corpus created during vectorization
    #topiccount - Int number of topics to generate the topic extraction model
    #method - topic extraction method to use (1=lsa,2=lda)
    
    #Use LSA or LDA
    if method == 1:
        model = LsiModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=topiccount,
        )
    elif method == 2:
        model = LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=topiccount,
            random_state=42,
            passes=10,
        )
    else:
        raise ValueError("method must be 'lsa' or 'lda'")

    # Print top words per topic
    print("\n=== Top words per topic ===")
    for idx, terms in model.show_topics(num_topics=topiccount, num_words=50, formatted=False):  
        if method == "lsa":
            # Keep only positive weights
            terms = [(word, prob) for word, prob in terms if prob > 0]
        # Take only the top N words after filtering
        terms = sorted(terms, key=lambda x: -x[1])[:relevanttopic]
        words = [word for word, _ in terms]
        print(f"Topic {idx+1}: {', '.join(words)}")
    return model

def sample_complaints(complaints, number_of_complaints):
    #complaints - All complaints
    #number_of_complaints - Int how many random complaints shall be returned
    
    if number_of_complaints >= len(complaints):
        # If requested more than available, return all complaints
        return complaints.copy()
    return random.sample(complaints, number_of_complaints)

def plot_coherence(coherence_values, start, limit, step):
    #coherence_values - list of coherence scores
    #start - Int topic count from coherence calculation
    #limit - Int topic count from coherence calculation
    #step - Int stepsize from coherence calculation
    
    #Plot Coherence Score Distribution
    x = list(range(start, limit + 1, step))
    plt.figure(figsize=(10,6))
    plt.plot(x, coherence_values, marker="o")
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence Score")
    plt.title("Coherence Score per Topic Count")
    plt.grid(True)
    plt.show()

def plot_top_words(model, num_topics, num_words, method):
    #model - the generated topic extraction model
    #num_topics - the number of topics used for the model
    #num_words - How many words describe a topic. Defined by user input.
    #method - topic extraction method used (1=LSA,2=LDA)

    for t in range(num_topics):
        #Get more words initially
        terms = model.show_topic(t, topn=50)

        if method == 1:
            #Keep only positive weights
            positive_terms = [(word, prob) for word, prob in terms if prob > 0]
            #If fewer than num_words, just take as many as exist
            terms = positive_terms[:num_words]
        else:
            terms = terms[:num_words]

        if not terms:
            print(f"Topic {t+1}: (no positive terms)")
            continue
        words, probs = zip(*terms)
        #Plot Top Words
        plt.figure(figsize=(8, 4))
        plt.barh(words, probs)
        plt.gca().invert_yaxis()
        plt.title(f"Topic {t+1}")
        plt.show()

def main():
    #variable initializationn
    complaints = []
    tokens = []
    vectormethod = None
    topicmethod = None
    wordspertopic = None
    numberofcomplaints = None
    #Words that appear often but do not add any value to the analysis
    customstopwords = ("xxxx","xxxxxxxx","xxxxxxxxxxxx","account","consumer")
    #For which topic counts the coherence score is calculated
    cohstart = 3
    cohstop = 25
    cohstep = 1
    #Name of the JSON field where to extract the complaints from
    jsonfieldname = "complaint_what_happened"

    print("Enter Path to input JSON")
    jsonfile = input()
    jsonfile = jsonfile.replace('"', '').replace("'", "")

    while vectormethod not in [1, 2]:
        print("Choose Vectorization method: 1=Bag of Words (Bow),2=Term Frequency-Inverse Document Frequency (TF-IDF)")
        vectormethod = int(input())

    while topicmethod not in [1, 2]:
        print("Choose topic extraction method: 1=Latent Semantic Analysis (LSA),2=Latent Dirichlet Allocation (LDA)")
        topicmethod = int(input())    

    while wordspertopic is None:
        try:
            user_input = input("How many words should be shown per topic? (default = 10): ").strip()
            if user_input == "":
                #Default value
                wordspertopic = 10   
            else:
                wordspertopic = int(user_input)
        except ValueError:
            print("Please enter a valid integer or press Enter for default.")
            wordspertopic = None

    while numberofcomplaints is None:
        try:
            user_input = input("How many random complaints shall be extracted from the JSON document (default = All): ").strip()
            if user_input == "":
                #Set default value to a extremly high number
                numberofcomplaints = 10**12   
            else:
                numberofcomplaints = int(user_input)
        except ValueError:
            print("Please enter a valid integer or press Enter for default.")
            numberofcomplaints = None

    #Download NLTK Data and initialize
    nltk.download("punkt")
    nltk.download("punkt_tab")
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('averaged_perceptron_tagger_eng')

    #Set English stop words. Add custom stop words
    stop_words = set(stopwords.words('english'))
    for stopword in customstopwords:
        stop_words.add(stopword)

    start = time.perf_counter()
    with open(jsonfile, "r") as f:
        data = json.load(f)
    end = time.perf_counter()
    print(f"Read JSON took {end - start:.4f} seconds")

    #If the jsonfieldname is not empty add the complaint to the complaints list
    start = time.perf_counter()
    for obj in data:
        if len(obj[jsonfieldname]) != 0:
            complaints.append(obj[jsonfieldname])
    end = time.perf_counter()
    print(f"Extract complaints took {end - start:.4f} seconds")

    #get the desired number of random complaints from all complaints
    complaints = sample_complaints(complaints,numberofcomplaints)
    print(str(len(complaints)) + " complaints will be used for analysis")

    #Preprocess complaints + convert complaints to tokens
    start = time.perf_counter()
    tokens = preprocess_parallel(complaints,stop_words)
    end = time.perf_counter()
    print(f"Preprocessing complaints took {end - start:.4f} seconds")

    #Vectorization. Convert tokens to vectors
    start = time.perf_counter()
    dict, corp, tokens_with_bigrams = newgensim_matrix(tokens, vectormethod)
    end = time.perf_counter()
    print(f"Vectorization took {end - start:.4f} seconds")

    #Compute the optimal amount of topics using the coherence score and plot the coherence distribution
    start = time.perf_counter()
    besttopiccount,coherence_val = compute_optimal_topics_parallel(tokens_with_bigrams,dict,corp,cohstart,cohstop,cohstep,topicmethod)
    plot_coherence(coherence_val, cohstart, cohstop, cohstep)
    end = time.perf_counter()
    print(f"Compute best topic count took {end - start:.4f} seconds")
    
    #Start topic extraction
    start = time.perf_counter()
    topicmodel = topic_extraction(dict, corp, besttopiccount, wordspertopic, topicmethod)
    end = time.perf_counter()
    print(f"Topic extraction took {end - start:.4f} seconds")

    #Plot the top words for each topic
    plot_top_words(topicmodel,besttopiccount,wordspertopic,topicmethod)
    print("Press Enter to end the program")
    input()

if __name__ == "__main__":
    main()